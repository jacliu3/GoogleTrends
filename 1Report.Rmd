---
title: "Analysis of the First Dataset"
author: "Jacqueline Liu"
date: "January 15, 2017"
output: 
  pdf_document:
    fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      fig.height = 3.5)
library(ggplot2)
library(TSA)
library(xtable)
library(RColorBrewer)

options(digits = 2, xtable.comment = FALSE)
theme_update(plot.title = element_text(hjust = 0.5))
```

## Exploratory Data Analysis
```{r eda}
ts1 <- ts(read.csv("1DS.csv", header = FALSE), frequency = 52)
t <- time(ts1)

ggplot(data = data.frame(), aes(x = t, y = ts1)) + geom_line() +
  xlab('Time') + ylab('Value') + ggtitle("Lineplot of Original Data")
```

The time series looks a little heteroskedastic (range of almost 50 in the first period, under 40 for the last), so a log transformation should be applied. The data clearly has a seasonal trend, in addition to the overall trend.

```{r log_transform}
ts1.log <- log(ts1)
```

## Cross-Validation
To compare models, we'll use the mean squared error given through cross validation. This function takes in the ARMA and the SARMA order as a list (p, d, q), along with _k_, the number of periods to predict. Larger _k_ have the disadvantage of having less data with which to build the model.
```{r cv}
computeCVmse <- function(order.totry, seasorder.totry, k = 1){
  MSE <- c()
  train.dt <-ts1.log[1:(length(ts1.log) - 52 * k)]
  test.dt <- ts1.log[(length(ts1.log) - 52 * k + 1):(length(ts1.log) - 52 * (k - 1))]
  mod <- arima(train.dt, order = order.totry, seasonal = 
                 list(order = seasorder.totry, period = 52))
  fcast <- predict(mod, n.ahead = 52)
  MSE <- mean((exp(fcast$pred) - exp(test.dt))^2)
  return(MSE)
}
```


## Nonparametric modeling
https://people.duke.edu/~rnau/arimrule.htm

The seasonality is likely 52, the number of weeks in a year, so the data should be differenced with that lag. 
```{r differencing}
ts1.diff <- diff(ts1.log, lag = 52)
```

```{r acf_comp, fig.cap = "ACF of original dataset"}
plot(acf(ts1, lag.max = 200, plot = FALSE), main = "Original ACF")
```
```{r acf_comp2, fig.cap = "ACF after log transformation and differencing"}
plot(acf(ts1.diff, lag.max = 200, plot = FALSE), main = "ACF of preprocessed data")
```

```{r, fig.cap = "Looking at the post-transformation ACF and PACF more closely"}
plot(acf(ts1.diff, plot = FALSE), main = "ACF plot")
plot(pacf(ts1.diff, plot = FALSE), main = "PACF plot")
```

Figure 2 looks much more noisy than Figure 1, and both plots in Figure 3 look like the result of a white noise process, suggesting the seasonal and linear trend were successfully removed. The ACF tails off while the PACF cuts off immediately, suggesting the data follows an AR(1) process. It could, however, also be a rough MA(3). Looking at the ACF over a greater number of lags in Figure 2, we see there is a peak after one period, but not after two or three. If the peak is significant, there could be a multiplicative SAR(1) or SMA(1) term in the model. 

```{r, eval = FALSE}
# AR(1) Models with SMA/SAR term
ar1 <- arima(ts1.log, order = c(1, 0, 0))
tsdiag(ar1)
AIC(ar1)
BIC(ar1)
ar1.error <- computeCVmse(c(1, 0, 0), c(0, 0, 0))

ma3 <- arima(ts1.log, order = c(0, 0, 3))
tsdiag(ar1)
AIC(ma3)
BIC(ma3)
ma3.error <- computeCVmse(c(0, 0, 3), c(0, 0, 0))

ar11 <- arima(ts1.log, order = c(1, 1, 0))
tsdiag(ar11)
AIC(ar11)
BIC(ar11)
ar11.error <- computeCVmse(c(1, 1, 0), c(0, 0, 0))

ma31 <- arima(ts1.log, order = c(0, 1, 3))
tsdiag(ar11)
AIC(ma31)
BIC(ma31)
ma31.error <- computeCVmse(c(0, 1, 3), c(0, 0, 0))

# AIC and BIC seem to prefer the differencing term for ma3, not for ar1
# But CV most definitely does not for either
# Implies original trend was linear, which we'd removed with the differencing

# Adding in the seasonal component
# No clear winner between AR1 and MA3, so we'll build off of both
# If only I could grid search this...
ar1.sar01 <- arima(ts1.log, order = c(1, 0, 0), 
              seasonal = list(order = c(0, 1, 0), period = 52))
AIC(ar1.sar01)
BIC(ar1.sar01 )
ar1.sar01.error <- computeCVmse(c(1, 0, 0), c(0, 1, 0))

ar11.sar01 <- arima(ts1.log, order = c(1, 1, 0), 
              seasonal = list(order = c(0, 1, 0), period = 52))
AIC(ar11.sar01)
BIC(ar11.sar01 )
ar11.sar01.error <- computeCVmse(c(1, 1, 0), c(0, 1, 0))

ar1.sar11 <- arima(ts1.log, order = c(1, 0, 0), 
              seasonal = list(order = c(1, 1, 0), period = 52))
AIC(ar1.sar11)
BIC(ar1.sar11 )
ar1.sar11.error <- computeCVmse(c(1, 0, 0), c(1, 1, 0))

ar11.sar11 <- arima(ts1.log, order = c(1, 1, 0), 
              seasonal = list(order = c(1, 1, 0), period = 52))
AIC(ar11.sar11)
BIC(ar11.sar11 )
ar11.sar11.error <- computeCVmse(c(1, 1, 0), c(1, 1, 0))

####
ma3.sar01 <- arima(ts1.log, order = c(0, 0, 3), 
              seasonal = list(order = c(0, 1, 0), period = 52))
AIC(ma3.sar01)
BIC(ma3.sar01 )
ma3.sar01.error <- computeCVmse(c(0, 0, 3), c(0, 1, 0))

ma31.sar01 <- arima(ts1.log, order = c(0, 1, 3), 
              seasonal = list(order = c(0, 1, 0), period = 52))
AIC(ma31.sar01)
BIC(ma31.sar01 )
ma31.sar01.error <- computeCVmse(c(0, 1, 3), c(0, 1, 0))

ma3.sar11 <- arima(ts1.log, order = c(0, 0, 3), 
              seasonal = list(order = c(1, 1, 0), period = 52))
AIC(ma3.sar31)
BIC(ma3.sar31 )
ma3.sar11.error <- computeCVmse(c(0, 0, 3), c(1, 1, 0))

ma31.sar11 <- arima(ts1.log, order = c(1, 1, 0), 
              seasonal = list(order = c(1, 1, 0), period = 52))
AIC(ma31.sar31)
BIC(ma31.sar31 )
ma31.sar11.error <- computeCVmse(c(0, 1, 3), c(1, 1, 0))

# MA31 with SAR01 or SAR11 seems to be doing best so far
# We'll test it with SMA11 to see if we can do any better

ma31.sma11 <- arima(ts1.log, order = c(0, 1, 3), 
              seasonal = list(order = c(0, 1, 1), period = 52))
AIC(ma31.sma1)
BIC(ma31.sma11 )
ma31.sma11.error <- computeCVmse(c(0, 1, 3), c(0, 1, 1))

# And it is better!
# Okay we're done here

models <- c("ar1", "ar11", "ma3", "ma31", "ar1.sar01", "ar1.sar11", "ar11.sar01", "ar11.sar11",
            "ma3.sar01", "ma3.sar11", "ma31.sar01", "ma31.sar11", "ma31.sma11")

AIC.results <- sapply(models, function(x) {AIC(get(x))})
BIC.results <- sapply(models, function(x) {BIC(get(x))})
CV.results <- sapply(lapply(models, function(x) {paste0(x, ".error")}), get)

model.statistics <- data.frame(AIC = AIC.results, BIC = BIC.results, CVerror = CV.results)

preds <- exp(predict(ma31.sma11, n.ahead = 52)$pred)
other <- exp(predict(ma31.sar11, n.ahead = 52)$pred)
more <- exp(predict(ar11.sar11, n.ahead = 52)$pred)

save(model.statistics, preds, other, more, file = "model_data.Rdata")
```
```{r}
load("model_data.Rdata")
```
Testing various combinations of AR(1) or MA(3) with SAR(1) or SMA(1) terms, along with a difference = 1 in the ARMA and the seasonal process, gave many different models. Comparing them with AIC, BIC, and CV-error statistics, the results are in Table 1. It seems that for models with lower cross-validation errors (i.e. under 90), the `ma31.sma11`, a SARMA(0, 1, 3)x(0, 1, 1) model, has the best AIC, BIC, and CV score, making it the obvious choice for prediction purposes. Using it to predict the next 52 datapoints, the results have to be raised to the power of $e$ because of the earlier log transformation.

```{r, results = "asis"}
xtable(model.statistics, caption = "Model Diagnostics")
```

```{r, fig.cap = "The given data is in red, ma31sma11 predictions in orange, ma31.sar11 green, ar11.sar11 blue"}
brewed.colors <- brewer.pal(4, "Spectral")
plot.data <- data.frame(x = c(t, rep(seq(5.02, 6, length.out = 52), 3)), 
                        y = c(ts1, preds, other, more))
ggplot( ) + 
  geom_line(data = plot.data[1:length(t), ], 
            aes(x = x, y = y), col = brewed.colors[1]) + 
  geom_line(data = plot.data[length(t):(length(t) + 52), ], 
            aes(x = x, y = y), col = brewed.colors[2]) + 
  geom_line(data = plot.data[(length(t) + 52):(length(t) + 2*52), ], 
            aes(x = x, y = y), col = brewed.colors[3], alpha = 0.7) + 
  geom_line(data = plot.data[(length(t) + 2*52):(length(t) + 3*52), ], 
            aes(x = x, y = y), col = brewed.colors[4], alpha = 0.7) + 
  xlab('Time') + ylab('Value') + ggtitle("Lineplot of Original Data and Predictions")

```






